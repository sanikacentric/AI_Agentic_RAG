{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-openai langchain-community faiss-cpu beautifulsoup4 requests"
      ],
      "metadata": {
        "id": "wGl2GtqcUaWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKX5hW5IQpqF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.agents import Tool\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain.agents import create_tool_calling_agent\n",
        "from langchain.tools import tool\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Initialize components\n",
        "class AgenticRAG:\n",
        "    def __init__(self):\n",
        "        # Set your OpenAI API key\n",
        "        os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
        "\n",
        "        # Initialize embedding model\n",
        "        self.embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "        # Initialize LLM\n",
        "        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "        # Initialize vector store\n",
        "        self.vector_store = None\n",
        "\n",
        "        # Initialize memory for conversation history\n",
        "        self.memory = []\n",
        "\n",
        "        # Initialize tools (can be extended)\n",
        "        self.tools = [\n",
        "            Tool(\n",
        "                name=\"web_search\",\n",
        "                func=self._web_search_tool,\n",
        "                description=\"Useful when you need to search the web for current information\"\n",
        "            ),\n",
        "            Tool(\n",
        "                name=\"memory\",\n",
        "                func=self._check_memory,\n",
        "                description=\"Useful when you need to recall previous conversations\"\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Create agent\n",
        "        self.agent = self._create_agent()\n",
        "\n",
        "    def _create_agent(self):\n",
        "        \"\"\"Create a LangChain agent with tools\"\"\"\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a helpful AI assistant with access to web search and memory.\n",
        "            Use the following tools when appropriate to answer questions accurately.\n",
        "            When using web search results, always cite your sources.\"\"\"),\n",
        "            (\"placeholder\", \"{chat_history}\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "            (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "        ])\n",
        "\n",
        "        agent = create_tool_calling_agent(self.llm, self.tools, prompt)\n",
        "        return AgentExecutor(agent=agent, tools=self.tools, verbose=True)\n",
        "\n",
        "    def fetch_and_index_content(self, urls):\n",
        "        \"\"\"Extraction Layer: Fetch and index content from URLs\"\"\"\n",
        "        try:\n",
        "            # Load documents\n",
        "            loader = WebBaseLoader(urls)\n",
        "            documents = loader.load()\n",
        "\n",
        "            # Check for llms.txt (AI equivalent of robots.txt)\n",
        "            for url in urls:\n",
        "                self._check_llms_txt(url)\n",
        "\n",
        "            # Split documents\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=1000,\n",
        "                chunk_overlap=200\n",
        "            )\n",
        "            splits = text_splitter.split_documents(documents)\n",
        "\n",
        "            # Create vector store\n",
        "            self.vector_store = FAISS.from_documents(\n",
        "                documents=splits,\n",
        "                embedding=self.embedding_model\n",
        "            )\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching content: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def _check_llms_txt(self, url):\n",
        "        \"\"\"Check for llms.txt file\"\"\"\n",
        "        try:\n",
        "            robots_url = url.rstrip('/') + '/llms.txt'\n",
        "            response = requests.get(robots_url, timeout=5)\n",
        "            if response.status_code == 200:\n",
        "                print(f\"Found llms.txt at {robots_url}\")\n",
        "                print(\"Content:\", response.text[:200] + \"...\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def query(self, user_query):\n",
        "        \"\"\"Process user query through the RAG pipeline\"\"\"\n",
        "        # First check if we should use the vector store or web search\n",
        "        if self.vector_store:\n",
        "            # Use RAG pipeline\n",
        "            response = self._rag_query(user_query)\n",
        "        else:\n",
        "            # Fall back to agent with web search\n",
        "            response = self.agent.invoke({\n",
        "                \"input\": user_query,\n",
        "                \"chat_history\": self.memory\n",
        "            })\n",
        "\n",
        "        # Update memory\n",
        "        self.memory.extend([\n",
        "            HumanMessage(content=user_query),\n",
        "            AIMessage(content=response if isinstance(response, str) else response.get('output', ''))\n",
        "        ])\n",
        "\n",
        "        return response\n",
        "\n",
        "    def _rag_query(self, query):\n",
        "        \"\"\"Full RAG pipeline with vector search\"\"\"\n",
        "        # Create retriever\n",
        "        retriever = self.vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "        # Define prompt template\n",
        "        template = \"\"\"Answer the question based only on the following context:\n",
        "        {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        If you don't know the answer, say you don't know.\n",
        "        Always cite your sources using the provided metadata.\"\"\"\n",
        "        prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "        # Create chain\n",
        "        chain = (\n",
        "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "            | prompt\n",
        "            | self.llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "        return chain.invoke(query)\n",
        "\n",
        "    @tool\n",
        "    def _web_search_tool(self, query):\n",
        "        \"\"\"Tool for web searching (simplified)\"\"\"\n",
        "        print(f\"Searching the web for: {query}\")\n",
        "        # In a real implementation, would use SERP API or similar\n",
        "        try:\n",
        "            # Mock some results\n",
        "            return \"Web search results for: \" + query + \"\\n1. Example result 1\\n2. Example result 2\"\n",
        "        except Exception as e:\n",
        "            return f\"Error performing web search: {str(e)}\"\n",
        "\n",
        "    @tool\n",
        "    def _check_memory(self):\n",
        "        \"\"\"Tool for checking conversation memory\"\"\"\n",
        "        if not self.memory:\n",
        "            return \"No previous conversations found in memory.\"\n",
        "\n",
        "        return \"Conversation history:\\n\" + \"\\n\".join(\n",
        "            f\"{msg.type}: {msg.content}\"\n",
        "            for msg in self.memory[-4:]  # Return last 4 messages\n",
        "        )\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    rag = AgenticRAG()\n",
        "\n",
        "    # Fetch and index some content\n",
        "    print(\"Fetching and indexing content...\")\n",
        "    rag.fetch_and_index_content([\n",
        "        \"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\",\n",
        "        \"https://en.wikipedia.org/wiki/Vector_database\"\n",
        "    ])\n",
        "\n",
        "    # Ask some questions\n",
        "    queries = [\n",
        "        \"What is RAG?\",\n",
        "        \"How does vector search work?\",\n",
        "        \"What are the benefits of using RAG with LLMs?\"\n",
        "    ]\n",
        "\n",
        "    for query in queries:\n",
        "        print(f\"\\nQuestion: {query}\")\n",
        "        response = rag.query(query)\n",
        "        print(f\"Answer: {response}\")\n",
        "\n",
        "    # Example using memory\n",
        "    print(\"\\nChecking memory...\")\n",
        "    print(rag._check_memory())"
      ]
    }
  ]
}